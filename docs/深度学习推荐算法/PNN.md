# PNN 模型——加强特征交叉能力

---

> 2016年，上海交通大学的研究人员提出了PNN模型，给出了多种特征交互方式的几种设计思路。
>
> 主要贡献：
>
> 基于product的结构，更好地学习category特征的embedding表示；
>
> 模型融合了category特征二阶交叉信息，以及使用全连接层学习高阶特征交叉信息。

## 模型结构

![PNN](https://blog-1252832257.cos.ap-shanghai.myqcloud.com/20445e3093ee4475b3722e8ee65e9d70.png)

PNN 模型相比之前的Deep Crossing 模型在结构上没有多大的不同，唯一的区别在于用Product layer代替了之前的Stacking Layer，不在是简单的embedding向量的连接。

相比NeuralCF，PNN模型的输入不仅包括用户和物品的信息，还可以有更多不同形式、来源的特征。PNN模型对深度学习结构的创新主要在于product layer中引入的特征交叉方式。

## 特征交叉方式

PNN模型的乘积层由线性操作部分（图中的z，对各特征向量进行线性拼接)和乘积操作部分（图中的p）。

乘积特征交叉部分又可分为内积操作和外积操作，使用内积的PNN模型成为IPNN，使用外积操作的PNN模型成为OPNN。为了保证乘积的顺利，embedding向量的维度需要保持一致。

内积操作：
$$
g_{inner}(f_i,f_j) = <f_i,f_j>
$$
外积操作：
$$
g_{outer}(f_i,f_j) = f_if_j^\top
$$
外积操作特征向量各维度两两交叉而成一个$M\times M$的矩阵，问题复杂度较高。

PNN模型的论文中提到一种降维的方法，把所有两两特征embedding向量外积互操作的结果叠加，形成一个叠加外积互操作矩阵p，其实质类似于让所有特征embedding向量通过一个平均池化层后，再进行外积互操作。但是这种池化操作把不同特征融合到一起，会模糊很多有价值的信息。通常池化操作可以发生在同类的embedding上。

PNN模型在经过线性和乘积操作后，在内部又**分别**进行了局部全连接层的转换，将维度映射到$D_1$维。

![image-20220712222248740](https://blog-1252832257.cos.ap-shanghai.myqcloud.com/image-20220712222248740.png)

## PNN模型的优势和局限性

PNN模型强调了特征embedding向量之间的操作，相比于之前Deep Crossing全交给全连接层进行无差别化的处理，PNN模型定义了乘积操作，使得不同特征之间产生交互。

但是，在外积操作的实际应用中，为了计算效率进行了大量的简化。此外，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征向量中包含的有价值信息。如何综合原始特征及交叉特征，让特征交叉的方式更加高校，后续的Wide&Deep模型和基于FM的深度学习模型将给出它们的解决方案。

# 代码实现

https://github.com/ZJYCP/awesome-rec/tree/main/code/PNN
